{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.944236919190562,
  "eval_steps": 500,
  "global_step": 23000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.021499019107253232,
      "grad_norm": 1.6940346956253052,
      "learning_rate": 9.957437661220981e-05,
      "loss": 5.4365,
      "step": 100
    },
    {
      "epoch": 0.042998038214506464,
      "grad_norm": 1.7953182458877563,
      "learning_rate": 9.914445399828032e-05,
      "loss": 4.7212,
      "step": 200
    },
    {
      "epoch": 0.0644970573217597,
      "grad_norm": 1.7616266012191772,
      "learning_rate": 9.871453138435082e-05,
      "loss": 4.5938,
      "step": 300
    },
    {
      "epoch": 0.08599607642901293,
      "grad_norm": 1.625588059425354,
      "learning_rate": 9.828460877042133e-05,
      "loss": 4.4914,
      "step": 400
    },
    {
      "epoch": 0.10749509553626616,
      "grad_norm": 1.4460644721984863,
      "learning_rate": 9.785468615649184e-05,
      "loss": 4.4282,
      "step": 500
    },
    {
      "epoch": 0.1289941146435194,
      "grad_norm": 1.6380155086517334,
      "learning_rate": 9.742476354256234e-05,
      "loss": 4.3701,
      "step": 600
    },
    {
      "epoch": 0.1504931337507726,
      "grad_norm": 1.4403902292251587,
      "learning_rate": 9.699484092863284e-05,
      "loss": 4.3418,
      "step": 700
    },
    {
      "epoch": 0.17199215285802585,
      "grad_norm": 1.4474358558654785,
      "learning_rate": 9.656491831470335e-05,
      "loss": 4.3002,
      "step": 800
    },
    {
      "epoch": 0.1934911719652791,
      "grad_norm": 1.6620032787322998,
      "learning_rate": 9.613499570077386e-05,
      "loss": 4.2831,
      "step": 900
    },
    {
      "epoch": 0.2149901910725323,
      "grad_norm": 1.5105763673782349,
      "learning_rate": 9.570507308684438e-05,
      "loss": 4.2568,
      "step": 1000
    },
    {
      "epoch": 0.23648921017978555,
      "grad_norm": 1.5868650674819946,
      "learning_rate": 9.527515047291487e-05,
      "loss": 4.2375,
      "step": 1100
    },
    {
      "epoch": 0.2579882292870388,
      "grad_norm": 1.482605218887329,
      "learning_rate": 9.484522785898539e-05,
      "loss": 4.2098,
      "step": 1200
    },
    {
      "epoch": 0.279487248394292,
      "grad_norm": 1.7057781219482422,
      "learning_rate": 9.44153052450559e-05,
      "loss": 4.2049,
      "step": 1300
    },
    {
      "epoch": 0.3009862675015452,
      "grad_norm": 1.4192297458648682,
      "learning_rate": 9.398538263112641e-05,
      "loss": 4.1659,
      "step": 1400
    },
    {
      "epoch": 0.3224852866087985,
      "grad_norm": 1.5392580032348633,
      "learning_rate": 9.355546001719691e-05,
      "loss": 4.1722,
      "step": 1500
    },
    {
      "epoch": 0.3439843057160517,
      "grad_norm": 1.5650767087936401,
      "learning_rate": 9.312553740326742e-05,
      "loss": 4.1633,
      "step": 1600
    },
    {
      "epoch": 0.3654833248233049,
      "grad_norm": 1.511350154876709,
      "learning_rate": 9.269561478933792e-05,
      "loss": 4.1649,
      "step": 1700
    },
    {
      "epoch": 0.3869823439305582,
      "grad_norm": 1.446540117263794,
      "learning_rate": 9.226569217540843e-05,
      "loss": 4.1167,
      "step": 1800
    },
    {
      "epoch": 0.4084813630378114,
      "grad_norm": 1.4054031372070312,
      "learning_rate": 9.183576956147893e-05,
      "loss": 4.1317,
      "step": 1900
    },
    {
      "epoch": 0.4299803821450646,
      "grad_norm": 1.4757601022720337,
      "learning_rate": 9.140584694754944e-05,
      "loss": 4.1268,
      "step": 2000
    },
    {
      "epoch": 0.45147940125231784,
      "grad_norm": 1.4023302793502808,
      "learning_rate": 9.097592433361996e-05,
      "loss": 4.1062,
      "step": 2100
    },
    {
      "epoch": 0.4729784203595711,
      "grad_norm": 1.3155940771102905,
      "learning_rate": 9.054600171969047e-05,
      "loss": 4.0992,
      "step": 2200
    },
    {
      "epoch": 0.4944774394668243,
      "grad_norm": 1.3843485116958618,
      "learning_rate": 9.011607910576097e-05,
      "loss": 4.0728,
      "step": 2300
    },
    {
      "epoch": 0.5159764585740776,
      "grad_norm": 1.489632248878479,
      "learning_rate": 8.968615649183148e-05,
      "loss": 4.0919,
      "step": 2400
    },
    {
      "epoch": 0.5374754776813307,
      "grad_norm": 1.4622617959976196,
      "learning_rate": 8.925623387790199e-05,
      "loss": 4.0647,
      "step": 2500
    },
    {
      "epoch": 0.558974496788584,
      "grad_norm": 1.4424432516098022,
      "learning_rate": 8.882631126397249e-05,
      "loss": 4.0701,
      "step": 2600
    },
    {
      "epoch": 0.5804735158958373,
      "grad_norm": 1.3146660327911377,
      "learning_rate": 8.839638865004299e-05,
      "loss": 4.0848,
      "step": 2700
    },
    {
      "epoch": 0.6019725350030904,
      "grad_norm": 1.479050874710083,
      "learning_rate": 8.79664660361135e-05,
      "loss": 4.0671,
      "step": 2800
    },
    {
      "epoch": 0.6234715541103437,
      "grad_norm": 1.5822243690490723,
      "learning_rate": 8.753654342218401e-05,
      "loss": 4.0635,
      "step": 2900
    },
    {
      "epoch": 0.644970573217597,
      "grad_norm": 1.455245852470398,
      "learning_rate": 8.710662080825451e-05,
      "loss": 4.0289,
      "step": 3000
    },
    {
      "epoch": 0.6664695923248501,
      "grad_norm": 1.4518181085586548,
      "learning_rate": 8.667669819432502e-05,
      "loss": 4.0423,
      "step": 3100
    },
    {
      "epoch": 0.6879686114321034,
      "grad_norm": 1.4638808965682983,
      "learning_rate": 8.624677558039553e-05,
      "loss": 4.021,
      "step": 3200
    },
    {
      "epoch": 0.7094676305393567,
      "grad_norm": 1.3183844089508057,
      "learning_rate": 8.581685296646605e-05,
      "loss": 4.0459,
      "step": 3300
    },
    {
      "epoch": 0.7309666496466098,
      "grad_norm": 1.5164356231689453,
      "learning_rate": 8.538693035253655e-05,
      "loss": 4.0197,
      "step": 3400
    },
    {
      "epoch": 0.7524656687538631,
      "grad_norm": 1.5385314226150513,
      "learning_rate": 8.495700773860706e-05,
      "loss": 4.0241,
      "step": 3500
    },
    {
      "epoch": 0.7739646878611164,
      "grad_norm": 1.424105167388916,
      "learning_rate": 8.452708512467757e-05,
      "loss": 4.0112,
      "step": 3600
    },
    {
      "epoch": 0.7954637069683695,
      "grad_norm": 1.6448715925216675,
      "learning_rate": 8.409716251074807e-05,
      "loss": 3.9901,
      "step": 3700
    },
    {
      "epoch": 0.8169627260756228,
      "grad_norm": 1.3740226030349731,
      "learning_rate": 8.366723989681857e-05,
      "loss": 3.9988,
      "step": 3800
    },
    {
      "epoch": 0.8384617451828761,
      "grad_norm": 1.4284573793411255,
      "learning_rate": 8.323731728288908e-05,
      "loss": 3.9819,
      "step": 3900
    },
    {
      "epoch": 0.8599607642901292,
      "grad_norm": 1.5482497215270996,
      "learning_rate": 8.280739466895959e-05,
      "loss": 3.9985,
      "step": 4000
    },
    {
      "epoch": 0.8814597833973825,
      "grad_norm": 1.4283533096313477,
      "learning_rate": 8.23774720550301e-05,
      "loss": 3.9862,
      "step": 4100
    },
    {
      "epoch": 0.9029588025046357,
      "grad_norm": 1.4174054861068726,
      "learning_rate": 8.19475494411006e-05,
      "loss": 3.9989,
      "step": 4200
    },
    {
      "epoch": 0.9244578216118889,
      "grad_norm": 1.3283616304397583,
      "learning_rate": 8.151762682717111e-05,
      "loss": 3.9838,
      "step": 4300
    },
    {
      "epoch": 0.9459568407191422,
      "grad_norm": 1.357975721359253,
      "learning_rate": 8.108770421324163e-05,
      "loss": 3.9965,
      "step": 4400
    },
    {
      "epoch": 0.9674558598263954,
      "grad_norm": 1.3571794033050537,
      "learning_rate": 8.065778159931214e-05,
      "loss": 3.9803,
      "step": 4500
    },
    {
      "epoch": 0.9889548789336486,
      "grad_norm": 1.4470206499099731,
      "learning_rate": 8.022785898538264e-05,
      "loss": 3.9695,
      "step": 4600
    },
    {
      "epoch": 1.0103195291714815,
      "grad_norm": 1.4176242351531982,
      "learning_rate": 7.979793637145314e-05,
      "loss": 3.9394,
      "step": 4700
    },
    {
      "epoch": 1.0318185482787348,
      "grad_norm": 1.5714802742004395,
      "learning_rate": 7.936801375752365e-05,
      "loss": 3.9145,
      "step": 4800
    },
    {
      "epoch": 1.053317567385988,
      "grad_norm": 1.5223182439804077,
      "learning_rate": 7.893809114359416e-05,
      "loss": 3.9268,
      "step": 4900
    },
    {
      "epoch": 1.0748165864932413,
      "grad_norm": 1.5602871179580688,
      "learning_rate": 7.850816852966466e-05,
      "loss": 3.9063,
      "step": 5000
    },
    {
      "epoch": 1.0963156056004946,
      "grad_norm": 1.683733344078064,
      "learning_rate": 7.807824591573517e-05,
      "loss": 3.8982,
      "step": 5100
    },
    {
      "epoch": 1.1178146247077476,
      "grad_norm": 1.674879550933838,
      "learning_rate": 7.764832330180568e-05,
      "loss": 3.9028,
      "step": 5200
    },
    {
      "epoch": 1.1393136438150009,
      "grad_norm": 1.5699063539505005,
      "learning_rate": 7.72184006878762e-05,
      "loss": 3.9143,
      "step": 5300
    },
    {
      "epoch": 1.1608126629222542,
      "grad_norm": 1.5587915182113647,
      "learning_rate": 7.678847807394669e-05,
      "loss": 3.8977,
      "step": 5400
    },
    {
      "epoch": 1.1823116820295074,
      "grad_norm": 1.624090552330017,
      "learning_rate": 7.63585554600172e-05,
      "loss": 3.9088,
      "step": 5500
    },
    {
      "epoch": 1.2038107011367607,
      "grad_norm": 1.5861300230026245,
      "learning_rate": 7.592863284608772e-05,
      "loss": 3.9361,
      "step": 5600
    },
    {
      "epoch": 1.225309720244014,
      "grad_norm": 1.5774624347686768,
      "learning_rate": 7.549871023215822e-05,
      "loss": 3.9054,
      "step": 5700
    },
    {
      "epoch": 1.246808739351267,
      "grad_norm": 1.547448992729187,
      "learning_rate": 7.506878761822871e-05,
      "loss": 3.9096,
      "step": 5800
    },
    {
      "epoch": 1.2683077584585203,
      "grad_norm": 1.5557515621185303,
      "learning_rate": 7.463886500429923e-05,
      "loss": 3.9167,
      "step": 5900
    },
    {
      "epoch": 1.2898067775657736,
      "grad_norm": 1.5384418964385986,
      "learning_rate": 7.420894239036974e-05,
      "loss": 3.8944,
      "step": 6000
    },
    {
      "epoch": 1.3113057966730268,
      "grad_norm": 1.7016640901565552,
      "learning_rate": 7.377901977644024e-05,
      "loss": 3.9136,
      "step": 6100
    },
    {
      "epoch": 1.33280481578028,
      "grad_norm": 1.4582679271697998,
      "learning_rate": 7.334909716251075e-05,
      "loss": 3.916,
      "step": 6200
    },
    {
      "epoch": 1.3543038348875331,
      "grad_norm": 1.5681151151657104,
      "learning_rate": 7.291917454858126e-05,
      "loss": 3.8902,
      "step": 6300
    },
    {
      "epoch": 1.3758028539947864,
      "grad_norm": 1.6219502687454224,
      "learning_rate": 7.248925193465177e-05,
      "loss": 3.8878,
      "step": 6400
    },
    {
      "epoch": 1.3973018731020397,
      "grad_norm": 1.5353487730026245,
      "learning_rate": 7.205932932072227e-05,
      "loss": 3.8992,
      "step": 6500
    },
    {
      "epoch": 1.418800892209293,
      "grad_norm": 1.526634693145752,
      "learning_rate": 7.162940670679278e-05,
      "loss": 3.8971,
      "step": 6600
    },
    {
      "epoch": 1.4402999113165462,
      "grad_norm": 1.5481516122817993,
      "learning_rate": 7.119948409286328e-05,
      "loss": 3.9001,
      "step": 6700
    },
    {
      "epoch": 1.4617989304237995,
      "grad_norm": 1.460039496421814,
      "learning_rate": 7.07695614789338e-05,
      "loss": 3.8841,
      "step": 6800
    },
    {
      "epoch": 1.4832979495310528,
      "grad_norm": 1.5534019470214844,
      "learning_rate": 7.03396388650043e-05,
      "loss": 3.8906,
      "step": 6900
    },
    {
      "epoch": 1.5047969686383058,
      "grad_norm": 1.5177279710769653,
      "learning_rate": 6.99097162510748e-05,
      "loss": 3.8683,
      "step": 7000
    },
    {
      "epoch": 1.526295987745559,
      "grad_norm": 1.485353946685791,
      "learning_rate": 6.947979363714532e-05,
      "loss": 3.8789,
      "step": 7100
    },
    {
      "epoch": 1.5477950068528123,
      "grad_norm": 1.7108242511749268,
      "learning_rate": 6.904987102321583e-05,
      "loss": 3.8678,
      "step": 7200
    },
    {
      "epoch": 1.5692940259600656,
      "grad_norm": 1.6513537168502808,
      "learning_rate": 6.861994840928633e-05,
      "loss": 3.8978,
      "step": 7300
    },
    {
      "epoch": 1.590793045067319,
      "grad_norm": 1.5609455108642578,
      "learning_rate": 6.819002579535684e-05,
      "loss": 3.8704,
      "step": 7400
    },
    {
      "epoch": 1.612292064174572,
      "grad_norm": 1.4963737726211548,
      "learning_rate": 6.776010318142735e-05,
      "loss": 3.8888,
      "step": 7500
    },
    {
      "epoch": 1.6337910832818254,
      "grad_norm": 1.4837015867233276,
      "learning_rate": 6.733018056749786e-05,
      "loss": 3.8807,
      "step": 7600
    },
    {
      "epoch": 1.6552901023890785,
      "grad_norm": 1.6171962022781372,
      "learning_rate": 6.690025795356836e-05,
      "loss": 3.8734,
      "step": 7700
    },
    {
      "epoch": 1.6767891214963317,
      "grad_norm": 1.6317108869552612,
      "learning_rate": 6.647033533963886e-05,
      "loss": 3.8714,
      "step": 7800
    },
    {
      "epoch": 1.698288140603585,
      "grad_norm": 1.6300938129425049,
      "learning_rate": 6.604041272570937e-05,
      "loss": 3.859,
      "step": 7900
    },
    {
      "epoch": 1.719787159710838,
      "grad_norm": 1.5326532125473022,
      "learning_rate": 6.561049011177989e-05,
      "loss": 3.8784,
      "step": 8000
    },
    {
      "epoch": 1.7412861788180916,
      "grad_norm": 1.7235207557678223,
      "learning_rate": 6.518056749785038e-05,
      "loss": 3.8696,
      "step": 8100
    },
    {
      "epoch": 1.7627851979253446,
      "grad_norm": 1.5010578632354736,
      "learning_rate": 6.47506448839209e-05,
      "loss": 3.8684,
      "step": 8200
    },
    {
      "epoch": 1.7842842170325979,
      "grad_norm": 1.4412416219711304,
      "learning_rate": 6.432072226999141e-05,
      "loss": 3.8684,
      "step": 8300
    },
    {
      "epoch": 1.8057832361398511,
      "grad_norm": 1.4722435474395752,
      "learning_rate": 6.389079965606192e-05,
      "loss": 3.8571,
      "step": 8400
    },
    {
      "epoch": 1.8272822552471042,
      "grad_norm": 1.5040154457092285,
      "learning_rate": 6.346087704213242e-05,
      "loss": 3.8856,
      "step": 8500
    },
    {
      "epoch": 1.8487812743543577,
      "grad_norm": 1.4614585638046265,
      "learning_rate": 6.303095442820293e-05,
      "loss": 3.8765,
      "step": 8600
    },
    {
      "epoch": 1.8702802934616107,
      "grad_norm": 1.6248522996902466,
      "learning_rate": 6.260103181427343e-05,
      "loss": 3.8737,
      "step": 8700
    },
    {
      "epoch": 1.891779312568864,
      "grad_norm": 1.5460312366485596,
      "learning_rate": 6.217110920034394e-05,
      "loss": 3.8707,
      "step": 8800
    },
    {
      "epoch": 1.9132783316761173,
      "grad_norm": 1.6098408699035645,
      "learning_rate": 6.174118658641444e-05,
      "loss": 3.8628,
      "step": 8900
    },
    {
      "epoch": 1.9347773507833705,
      "grad_norm": 1.6722816228866577,
      "learning_rate": 6.131126397248495e-05,
      "loss": 3.8575,
      "step": 9000
    },
    {
      "epoch": 1.9562763698906238,
      "grad_norm": 1.6264376640319824,
      "learning_rate": 6.0881341358555465e-05,
      "loss": 3.8642,
      "step": 9100
    },
    {
      "epoch": 1.9777753889978769,
      "grad_norm": 1.443044662475586,
      "learning_rate": 6.0451418744625964e-05,
      "loss": 3.8633,
      "step": 9200
    },
    {
      "epoch": 1.9992744081051304,
      "grad_norm": 1.594171404838562,
      "learning_rate": 6.0021496130696476e-05,
      "loss": 3.8778,
      "step": 9300
    },
    {
      "epoch": 2.020639058342963,
      "grad_norm": 1.5880597829818726,
      "learning_rate": 5.959157351676699e-05,
      "loss": 3.7797,
      "step": 9400
    },
    {
      "epoch": 2.0421380774502165,
      "grad_norm": 1.6359357833862305,
      "learning_rate": 5.916165090283749e-05,
      "loss": 3.7802,
      "step": 9500
    },
    {
      "epoch": 2.0636370965574695,
      "grad_norm": 1.811457872390747,
      "learning_rate": 5.873172828890799e-05,
      "loss": 3.763,
      "step": 9600
    },
    {
      "epoch": 2.085136115664723,
      "grad_norm": 1.6561063528060913,
      "learning_rate": 5.8301805674978504e-05,
      "loss": 3.7798,
      "step": 9700
    },
    {
      "epoch": 2.106635134771976,
      "grad_norm": 1.78513765335083,
      "learning_rate": 5.7871883061049016e-05,
      "loss": 3.785,
      "step": 9800
    },
    {
      "epoch": 2.128134153879229,
      "grad_norm": 1.7190948724746704,
      "learning_rate": 5.744196044711953e-05,
      "loss": 3.7907,
      "step": 9900
    },
    {
      "epoch": 2.1496331729864826,
      "grad_norm": 1.7375819683074951,
      "learning_rate": 5.701203783319003e-05,
      "loss": 3.8075,
      "step": 10000
    },
    {
      "epoch": 2.1711321920937356,
      "grad_norm": 1.8088122606277466,
      "learning_rate": 5.658211521926053e-05,
      "loss": 3.7839,
      "step": 10100
    },
    {
      "epoch": 2.192631211200989,
      "grad_norm": 1.7948988676071167,
      "learning_rate": 5.6152192605331044e-05,
      "loss": 3.7862,
      "step": 10200
    },
    {
      "epoch": 2.214130230308242,
      "grad_norm": 1.6460950374603271,
      "learning_rate": 5.5722269991401556e-05,
      "loss": 3.7928,
      "step": 10300
    },
    {
      "epoch": 2.2356292494154952,
      "grad_norm": 1.755173683166504,
      "learning_rate": 5.5292347377472055e-05,
      "loss": 3.7922,
      "step": 10400
    },
    {
      "epoch": 2.2571282685227487,
      "grad_norm": 1.8204938173294067,
      "learning_rate": 5.486242476354256e-05,
      "loss": 3.7784,
      "step": 10500
    },
    {
      "epoch": 2.2786272876300018,
      "grad_norm": 1.6893794536590576,
      "learning_rate": 5.443250214961307e-05,
      "loss": 3.805,
      "step": 10600
    },
    {
      "epoch": 2.3001263067372553,
      "grad_norm": 1.896741271018982,
      "learning_rate": 5.4002579535683585e-05,
      "loss": 3.7965,
      "step": 10700
    },
    {
      "epoch": 2.3216253258445083,
      "grad_norm": 1.8803634643554688,
      "learning_rate": 5.357265692175408e-05,
      "loss": 3.7763,
      "step": 10800
    },
    {
      "epoch": 2.343124344951762,
      "grad_norm": 1.8590843677520752,
      "learning_rate": 5.3142734307824595e-05,
      "loss": 3.7975,
      "step": 10900
    },
    {
      "epoch": 2.364623364059015,
      "grad_norm": 1.844931960105896,
      "learning_rate": 5.27128116938951e-05,
      "loss": 3.807,
      "step": 11000
    },
    {
      "epoch": 2.386122383166268,
      "grad_norm": 1.75056791305542,
      "learning_rate": 5.228288907996561e-05,
      "loss": 3.7878,
      "step": 11100
    },
    {
      "epoch": 2.4076214022735214,
      "grad_norm": 1.8465237617492676,
      "learning_rate": 5.185296646603611e-05,
      "loss": 3.7786,
      "step": 11200
    },
    {
      "epoch": 2.4291204213807744,
      "grad_norm": 1.6596163511276245,
      "learning_rate": 5.1423043852106624e-05,
      "loss": 3.7725,
      "step": 11300
    },
    {
      "epoch": 2.450619440488028,
      "grad_norm": 1.9463939666748047,
      "learning_rate": 5.0993121238177136e-05,
      "loss": 3.7952,
      "step": 11400
    },
    {
      "epoch": 2.472118459595281,
      "grad_norm": 1.822863221168518,
      "learning_rate": 5.056319862424764e-05,
      "loss": 3.7901,
      "step": 11500
    },
    {
      "epoch": 2.493617478702534,
      "grad_norm": 1.779998779296875,
      "learning_rate": 5.013327601031814e-05,
      "loss": 3.7746,
      "step": 11600
    },
    {
      "epoch": 2.5151164978097875,
      "grad_norm": 1.6618878841400146,
      "learning_rate": 4.970335339638865e-05,
      "loss": 3.7795,
      "step": 11700
    },
    {
      "epoch": 2.5366155169170406,
      "grad_norm": 1.9375499486923218,
      "learning_rate": 4.9273430782459164e-05,
      "loss": 3.7781,
      "step": 11800
    },
    {
      "epoch": 2.558114536024294,
      "grad_norm": 1.7416359186172485,
      "learning_rate": 4.884350816852967e-05,
      "loss": 3.7803,
      "step": 11900
    },
    {
      "epoch": 2.579613555131547,
      "grad_norm": 1.5622384548187256,
      "learning_rate": 4.8413585554600175e-05,
      "loss": 3.805,
      "step": 12000
    },
    {
      "epoch": 2.6011125742388,
      "grad_norm": 1.7818701267242432,
      "learning_rate": 4.798366294067068e-05,
      "loss": 3.7725,
      "step": 12100
    },
    {
      "epoch": 2.6226115933460536,
      "grad_norm": 1.7072821855545044,
      "learning_rate": 4.7553740326741185e-05,
      "loss": 3.772,
      "step": 12200
    },
    {
      "epoch": 2.6441106124533067,
      "grad_norm": 1.895997405052185,
      "learning_rate": 4.71238177128117e-05,
      "loss": 3.7847,
      "step": 12300
    },
    {
      "epoch": 2.66560963156056,
      "grad_norm": 1.6297186613082886,
      "learning_rate": 4.66938950988822e-05,
      "loss": 3.7921,
      "step": 12400
    },
    {
      "epoch": 2.6871086506678132,
      "grad_norm": 1.7346078157424927,
      "learning_rate": 4.626397248495271e-05,
      "loss": 3.7776,
      "step": 12500
    },
    {
      "epoch": 2.7086076697750663,
      "grad_norm": 1.8202729225158691,
      "learning_rate": 4.583404987102321e-05,
      "loss": 3.7901,
      "step": 12600
    },
    {
      "epoch": 2.7301066888823198,
      "grad_norm": 2.0684518814086914,
      "learning_rate": 4.5404127257093726e-05,
      "loss": 3.7749,
      "step": 12700
    },
    {
      "epoch": 2.751605707989573,
      "grad_norm": 1.7115172147750854,
      "learning_rate": 4.497420464316423e-05,
      "loss": 3.7879,
      "step": 12800
    },
    {
      "epoch": 2.7731047270968263,
      "grad_norm": 1.8532778024673462,
      "learning_rate": 4.454428202923474e-05,
      "loss": 3.7798,
      "step": 12900
    },
    {
      "epoch": 2.7946037462040794,
      "grad_norm": 1.856431484222412,
      "learning_rate": 4.411435941530525e-05,
      "loss": 3.7774,
      "step": 13000
    },
    {
      "epoch": 2.8161027653113324,
      "grad_norm": 1.7040807008743286,
      "learning_rate": 4.3684436801375754e-05,
      "loss": 3.7699,
      "step": 13100
    },
    {
      "epoch": 2.837601784418586,
      "grad_norm": 1.629120945930481,
      "learning_rate": 4.325451418744626e-05,
      "loss": 3.792,
      "step": 13200
    },
    {
      "epoch": 2.8591008035258394,
      "grad_norm": 1.8252118825912476,
      "learning_rate": 4.282459157351677e-05,
      "loss": 3.7687,
      "step": 13300
    },
    {
      "epoch": 2.8805998226330924,
      "grad_norm": 1.8070074319839478,
      "learning_rate": 4.2394668959587276e-05,
      "loss": 3.7793,
      "step": 13400
    },
    {
      "epoch": 2.9020988417403455,
      "grad_norm": 1.8491313457489014,
      "learning_rate": 4.196474634565778e-05,
      "loss": 3.7786,
      "step": 13500
    },
    {
      "epoch": 2.923597860847599,
      "grad_norm": 1.733498454093933,
      "learning_rate": 4.153482373172829e-05,
      "loss": 3.7795,
      "step": 13600
    },
    {
      "epoch": 2.945096879954852,
      "grad_norm": 1.8662936687469482,
      "learning_rate": 4.11049011177988e-05,
      "loss": 3.7857,
      "step": 13700
    },
    {
      "epoch": 2.9665958990621055,
      "grad_norm": 1.8703258037567139,
      "learning_rate": 4.0674978503869305e-05,
      "loss": 3.7768,
      "step": 13800
    },
    {
      "epoch": 2.9880949181693586,
      "grad_norm": 1.7683625221252441,
      "learning_rate": 4.024505588993982e-05,
      "loss": 3.7907,
      "step": 13900
    },
    {
      "epoch": 3.0094595684071916,
      "grad_norm": 1.8070043325424194,
      "learning_rate": 3.981513327601032e-05,
      "loss": 3.7457,
      "step": 14000
    },
    {
      "epoch": 3.0309585875144447,
      "grad_norm": 1.9237736463546753,
      "learning_rate": 3.938521066208083e-05,
      "loss": 3.7213,
      "step": 14100
    },
    {
      "epoch": 3.0524576066216977,
      "grad_norm": 1.863608717918396,
      "learning_rate": 3.895528804815133e-05,
      "loss": 3.7169,
      "step": 14200
    },
    {
      "epoch": 3.0739566257289512,
      "grad_norm": 1.9936672449111938,
      "learning_rate": 3.8525365434221845e-05,
      "loss": 3.7093,
      "step": 14300
    },
    {
      "epoch": 3.0954556448362043,
      "grad_norm": 1.872451663017273,
      "learning_rate": 3.809544282029235e-05,
      "loss": 3.7084,
      "step": 14400
    },
    {
      "epoch": 3.1169546639434578,
      "grad_norm": 2.0345818996429443,
      "learning_rate": 3.7665520206362856e-05,
      "loss": 3.7108,
      "step": 14500
    },
    {
      "epoch": 3.138453683050711,
      "grad_norm": 2.0677084922790527,
      "learning_rate": 3.723559759243336e-05,
      "loss": 3.6906,
      "step": 14600
    },
    {
      "epoch": 3.159952702157964,
      "grad_norm": 1.9078824520111084,
      "learning_rate": 3.680567497850387e-05,
      "loss": 3.708,
      "step": 14700
    },
    {
      "epoch": 3.1814517212652174,
      "grad_norm": 1.9967256784439087,
      "learning_rate": 3.637575236457438e-05,
      "loss": 3.6982,
      "step": 14800
    },
    {
      "epoch": 3.2029507403724704,
      "grad_norm": 2.0519819259643555,
      "learning_rate": 3.594582975064489e-05,
      "loss": 3.7148,
      "step": 14900
    },
    {
      "epoch": 3.224449759479724,
      "grad_norm": 2.188995122909546,
      "learning_rate": 3.5515907136715396e-05,
      "loss": 3.7129,
      "step": 15000
    },
    {
      "epoch": 3.245948778586977,
      "grad_norm": 1.956728458404541,
      "learning_rate": 3.50859845227859e-05,
      "loss": 3.7074,
      "step": 15100
    },
    {
      "epoch": 3.2674477976942304,
      "grad_norm": 1.9124425649642944,
      "learning_rate": 3.465606190885641e-05,
      "loss": 3.7033,
      "step": 15200
    },
    {
      "epoch": 3.2889468168014835,
      "grad_norm": 1.9247376918792725,
      "learning_rate": 3.422613929492691e-05,
      "loss": 3.7083,
      "step": 15300
    },
    {
      "epoch": 3.3104458359087365,
      "grad_norm": 2.1507391929626465,
      "learning_rate": 3.3796216680997424e-05,
      "loss": 3.7149,
      "step": 15400
    },
    {
      "epoch": 3.33194485501599,
      "grad_norm": 2.111966848373413,
      "learning_rate": 3.336629406706793e-05,
      "loss": 3.7122,
      "step": 15500
    },
    {
      "epoch": 3.353443874123243,
      "grad_norm": 2.034451961517334,
      "learning_rate": 3.2936371453138435e-05,
      "loss": 3.7071,
      "step": 15600
    },
    {
      "epoch": 3.3749428932304966,
      "grad_norm": 2.0109221935272217,
      "learning_rate": 3.250644883920894e-05,
      "loss": 3.7331,
      "step": 15700
    },
    {
      "epoch": 3.3964419123377496,
      "grad_norm": 2.0992729663848877,
      "learning_rate": 3.207652622527945e-05,
      "loss": 3.7083,
      "step": 15800
    },
    {
      "epoch": 3.4179409314450027,
      "grad_norm": 1.947873830795288,
      "learning_rate": 3.164660361134996e-05,
      "loss": 3.7241,
      "step": 15900
    },
    {
      "epoch": 3.439439950552256,
      "grad_norm": 2.1142632961273193,
      "learning_rate": 3.121668099742047e-05,
      "loss": 3.7076,
      "step": 16000
    },
    {
      "epoch": 3.460938969659509,
      "grad_norm": 1.9406867027282715,
      "learning_rate": 3.078675838349097e-05,
      "loss": 3.719,
      "step": 16100
    },
    {
      "epoch": 3.4824379887667627,
      "grad_norm": 1.9700512886047363,
      "learning_rate": 3.035683576956148e-05,
      "loss": 3.7259,
      "step": 16200
    },
    {
      "epoch": 3.5039370078740157,
      "grad_norm": 1.931693434715271,
      "learning_rate": 2.9926913155631986e-05,
      "loss": 3.6923,
      "step": 16300
    },
    {
      "epoch": 3.525436026981269,
      "grad_norm": 1.8135762214660645,
      "learning_rate": 2.9496990541702495e-05,
      "loss": 3.7238,
      "step": 16400
    },
    {
      "epoch": 3.5469350460885223,
      "grad_norm": 2.0494565963745117,
      "learning_rate": 2.9067067927773e-05,
      "loss": 3.7229,
      "step": 16500
    },
    {
      "epoch": 3.5684340651957753,
      "grad_norm": 2.031787157058716,
      "learning_rate": 2.8637145313843512e-05,
      "loss": 3.7098,
      "step": 16600
    },
    {
      "epoch": 3.589933084303029,
      "grad_norm": 2.0869534015655518,
      "learning_rate": 2.8207222699914014e-05,
      "loss": 3.7071,
      "step": 16700
    },
    {
      "epoch": 3.611432103410282,
      "grad_norm": 1.9915192127227783,
      "learning_rate": 2.7777300085984526e-05,
      "loss": 3.7096,
      "step": 16800
    },
    {
      "epoch": 3.632931122517535,
      "grad_norm": 2.0902211666107178,
      "learning_rate": 2.734737747205503e-05,
      "loss": 3.7166,
      "step": 16900
    },
    {
      "epoch": 3.6544301416247884,
      "grad_norm": 1.8605492115020752,
      "learning_rate": 2.691745485812554e-05,
      "loss": 3.709,
      "step": 17000
    },
    {
      "epoch": 3.6759291607320415,
      "grad_norm": 2.1298303604125977,
      "learning_rate": 2.6487532244196046e-05,
      "loss": 3.7242,
      "step": 17100
    },
    {
      "epoch": 3.697428179839295,
      "grad_norm": 2.0264229774475098,
      "learning_rate": 2.6057609630266554e-05,
      "loss": 3.7191,
      "step": 17200
    },
    {
      "epoch": 3.718927198946548,
      "grad_norm": 1.914491891860962,
      "learning_rate": 2.562768701633706e-05,
      "loss": 3.7019,
      "step": 17300
    },
    {
      "epoch": 3.740426218053801,
      "grad_norm": 2.058551788330078,
      "learning_rate": 2.519776440240757e-05,
      "loss": 3.7064,
      "step": 17400
    },
    {
      "epoch": 3.7619252371610545,
      "grad_norm": 2.2193188667297363,
      "learning_rate": 2.4767841788478074e-05,
      "loss": 3.7087,
      "step": 17500
    },
    {
      "epoch": 3.783424256268308,
      "grad_norm": 2.2255539894104004,
      "learning_rate": 2.4337919174548582e-05,
      "loss": 3.7046,
      "step": 17600
    },
    {
      "epoch": 3.804923275375561,
      "grad_norm": 1.8671848773956299,
      "learning_rate": 2.3907996560619088e-05,
      "loss": 3.7185,
      "step": 17700
    },
    {
      "epoch": 3.826422294482814,
      "grad_norm": 1.9429917335510254,
      "learning_rate": 2.3478073946689596e-05,
      "loss": 3.7115,
      "step": 17800
    },
    {
      "epoch": 3.8479213135900676,
      "grad_norm": 2.263090133666992,
      "learning_rate": 2.3048151332760105e-05,
      "loss": 3.7291,
      "step": 17900
    },
    {
      "epoch": 3.8694203326973207,
      "grad_norm": 2.0748844146728516,
      "learning_rate": 2.261822871883061e-05,
      "loss": 3.7104,
      "step": 18000
    },
    {
      "epoch": 3.890919351804574,
      "grad_norm": 1.9435789585113525,
      "learning_rate": 2.218830610490112e-05,
      "loss": 3.7186,
      "step": 18100
    },
    {
      "epoch": 3.912418370911827,
      "grad_norm": 1.925391435623169,
      "learning_rate": 2.1758383490971625e-05,
      "loss": 3.6955,
      "step": 18200
    },
    {
      "epoch": 3.9339173900190803,
      "grad_norm": 1.943237066268921,
      "learning_rate": 2.1328460877042133e-05,
      "loss": 3.7114,
      "step": 18300
    },
    {
      "epoch": 3.9554164091263337,
      "grad_norm": 1.8583366870880127,
      "learning_rate": 2.0898538263112642e-05,
      "loss": 3.699,
      "step": 18400
    },
    {
      "epoch": 3.976915428233587,
      "grad_norm": 2.076472759246826,
      "learning_rate": 2.0468615649183147e-05,
      "loss": 3.7113,
      "step": 18500
    },
    {
      "epoch": 3.9984144473408403,
      "grad_norm": 1.9646880626678467,
      "learning_rate": 2.0038693035253656e-05,
      "loss": 3.7057,
      "step": 18600
    },
    {
      "epoch": 4.019779097578673,
      "grad_norm": 2.2650763988494873,
      "learning_rate": 1.960877042132416e-05,
      "loss": 3.6643,
      "step": 18700
    },
    {
      "epoch": 4.041278116685926,
      "grad_norm": 2.099966526031494,
      "learning_rate": 1.917884780739467e-05,
      "loss": 3.6594,
      "step": 18800
    },
    {
      "epoch": 4.062777135793179,
      "grad_norm": 2.0922088623046875,
      "learning_rate": 1.874892519346518e-05,
      "loss": 3.6468,
      "step": 18900
    },
    {
      "epoch": 4.084276154900433,
      "grad_norm": 2.0294620990753174,
      "learning_rate": 1.8319002579535684e-05,
      "loss": 3.6578,
      "step": 19000
    },
    {
      "epoch": 4.105775174007686,
      "grad_norm": 2.173067092895508,
      "learning_rate": 1.7889079965606193e-05,
      "loss": 3.6609,
      "step": 19100
    },
    {
      "epoch": 4.127274193114939,
      "grad_norm": 2.1031994819641113,
      "learning_rate": 1.74591573516767e-05,
      "loss": 3.6498,
      "step": 19200
    },
    {
      "epoch": 4.148773212222192,
      "grad_norm": 2.169034719467163,
      "learning_rate": 1.7029234737747207e-05,
      "loss": 3.6674,
      "step": 19300
    },
    {
      "epoch": 4.170272231329446,
      "grad_norm": 2.1345367431640625,
      "learning_rate": 1.6599312123817716e-05,
      "loss": 3.6738,
      "step": 19400
    },
    {
      "epoch": 4.191771250436699,
      "grad_norm": 2.2323451042175293,
      "learning_rate": 1.616938950988822e-05,
      "loss": 3.6414,
      "step": 19500
    },
    {
      "epoch": 4.213270269543952,
      "grad_norm": 2.264296770095825,
      "learning_rate": 1.573946689595873e-05,
      "loss": 3.6283,
      "step": 19600
    },
    {
      "epoch": 4.234769288651205,
      "grad_norm": 2.15457820892334,
      "learning_rate": 1.5309544282029235e-05,
      "loss": 3.6769,
      "step": 19700
    },
    {
      "epoch": 4.256268307758458,
      "grad_norm": 2.268092632293701,
      "learning_rate": 1.4879621668099744e-05,
      "loss": 3.6525,
      "step": 19800
    },
    {
      "epoch": 4.277767326865712,
      "grad_norm": 2.139247417449951,
      "learning_rate": 1.444969905417025e-05,
      "loss": 3.6438,
      "step": 19900
    },
    {
      "epoch": 4.299266345972965,
      "grad_norm": 2.146111488342285,
      "learning_rate": 1.4019776440240757e-05,
      "loss": 3.6454,
      "step": 20000
    },
    {
      "epoch": 4.320765365080218,
      "grad_norm": 2.421797275543213,
      "learning_rate": 1.3589853826311264e-05,
      "loss": 3.6604,
      "step": 20100
    },
    {
      "epoch": 4.342264384187471,
      "grad_norm": 2.2094085216522217,
      "learning_rate": 1.315993121238177e-05,
      "loss": 3.65,
      "step": 20200
    },
    {
      "epoch": 4.363763403294724,
      "grad_norm": 2.2457942962646484,
      "learning_rate": 1.2730008598452278e-05,
      "loss": 3.6616,
      "step": 20300
    },
    {
      "epoch": 4.385262422401978,
      "grad_norm": 2.2518250942230225,
      "learning_rate": 1.2300085984522786e-05,
      "loss": 3.6657,
      "step": 20400
    },
    {
      "epoch": 4.406761441509231,
      "grad_norm": 2.0673112869262695,
      "learning_rate": 1.1870163370593295e-05,
      "loss": 3.6648,
      "step": 20500
    },
    {
      "epoch": 4.428260460616484,
      "grad_norm": 2.22800612449646,
      "learning_rate": 1.1440240756663802e-05,
      "loss": 3.6877,
      "step": 20600
    },
    {
      "epoch": 4.449759479723737,
      "grad_norm": 2.0891053676605225,
      "learning_rate": 1.1010318142734307e-05,
      "loss": 3.6601,
      "step": 20700
    },
    {
      "epoch": 4.4712584988309905,
      "grad_norm": 2.0331921577453613,
      "learning_rate": 1.0580395528804815e-05,
      "loss": 3.6583,
      "step": 20800
    },
    {
      "epoch": 4.492757517938244,
      "grad_norm": 2.2556495666503906,
      "learning_rate": 1.0150472914875323e-05,
      "loss": 3.6547,
      "step": 20900
    },
    {
      "epoch": 4.5142565370454975,
      "grad_norm": 2.240203380584717,
      "learning_rate": 9.72055030094583e-06,
      "loss": 3.6758,
      "step": 21000
    },
    {
      "epoch": 4.5357555561527505,
      "grad_norm": 2.2588815689086914,
      "learning_rate": 9.290627687016337e-06,
      "loss": 3.6521,
      "step": 21100
    },
    {
      "epoch": 4.5572545752600035,
      "grad_norm": 2.269167184829712,
      "learning_rate": 8.860705073086844e-06,
      "loss": 3.6684,
      "step": 21200
    },
    {
      "epoch": 4.578753594367257,
      "grad_norm": 2.3071534633636475,
      "learning_rate": 8.430782459157351e-06,
      "loss": 3.6428,
      "step": 21300
    },
    {
      "epoch": 4.6002526134745105,
      "grad_norm": 2.191535472869873,
      "learning_rate": 8.00085984522786e-06,
      "loss": 3.6584,
      "step": 21400
    },
    {
      "epoch": 4.621751632581764,
      "grad_norm": 2.3539178371429443,
      "learning_rate": 7.570937231298366e-06,
      "loss": 3.6549,
      "step": 21500
    },
    {
      "epoch": 4.643250651689017,
      "grad_norm": 2.3149361610412598,
      "learning_rate": 7.141014617368874e-06,
      "loss": 3.657,
      "step": 21600
    },
    {
      "epoch": 4.66474967079627,
      "grad_norm": 2.1874806880950928,
      "learning_rate": 6.711092003439381e-06,
      "loss": 3.656,
      "step": 21700
    },
    {
      "epoch": 4.686248689903524,
      "grad_norm": 2.1244189739227295,
      "learning_rate": 6.281169389509889e-06,
      "loss": 3.6544,
      "step": 21800
    },
    {
      "epoch": 4.707747709010777,
      "grad_norm": 2.2521467208862305,
      "learning_rate": 5.851246775580395e-06,
      "loss": 3.6565,
      "step": 21900
    },
    {
      "epoch": 4.72924672811803,
      "grad_norm": 2.160106658935547,
      "learning_rate": 5.421324161650903e-06,
      "loss": 3.6628,
      "step": 22000
    },
    {
      "epoch": 4.750745747225283,
      "grad_norm": 2.177175521850586,
      "learning_rate": 4.99140154772141e-06,
      "loss": 3.6462,
      "step": 22100
    },
    {
      "epoch": 4.772244766332536,
      "grad_norm": 2.132638692855835,
      "learning_rate": 4.561478933791917e-06,
      "loss": 3.6518,
      "step": 22200
    },
    {
      "epoch": 4.793743785439789,
      "grad_norm": 2.3160758018493652,
      "learning_rate": 4.131556319862425e-06,
      "loss": 3.6446,
      "step": 22300
    },
    {
      "epoch": 4.815242804547043,
      "grad_norm": 2.232858657836914,
      "learning_rate": 3.7016337059329323e-06,
      "loss": 3.661,
      "step": 22400
    },
    {
      "epoch": 4.836741823654296,
      "grad_norm": 2.2214934825897217,
      "learning_rate": 3.2717110920034398e-06,
      "loss": 3.6632,
      "step": 22500
    },
    {
      "epoch": 4.858240842761549,
      "grad_norm": 2.314577579498291,
      "learning_rate": 2.841788478073947e-06,
      "loss": 3.6606,
      "step": 22600
    },
    {
      "epoch": 4.879739861868802,
      "grad_norm": 2.3384130001068115,
      "learning_rate": 2.4118658641444543e-06,
      "loss": 3.6661,
      "step": 22700
    },
    {
      "epoch": 4.901238880976056,
      "grad_norm": 2.24710750579834,
      "learning_rate": 1.9819432502149613e-06,
      "loss": 3.6528,
      "step": 22800
    },
    {
      "epoch": 4.922737900083309,
      "grad_norm": 2.2740769386291504,
      "learning_rate": 1.5520206362854686e-06,
      "loss": 3.6608,
      "step": 22900
    },
    {
      "epoch": 4.944236919190562,
      "grad_norm": 2.120059013366699,
      "learning_rate": 1.122098022355976e-06,
      "loss": 3.6777,
      "step": 23000
    }
  ],
  "logging_steps": 100,
  "max_steps": 23260,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0999575618823127e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
