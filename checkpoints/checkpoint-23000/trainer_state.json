{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.944236919190562,
  "eval_steps": 500,
  "global_step": 23000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.021499019107253232,
      "grad_norm": 1.9347723722457886,
      "learning_rate": 9.957437661220981e-05,
      "loss": 5.3333,
      "step": 100
    },
    {
      "epoch": 0.042998038214506464,
      "grad_norm": 1.8477715253829956,
      "learning_rate": 9.914445399828032e-05,
      "loss": 4.6397,
      "step": 200
    },
    {
      "epoch": 0.0644970573217597,
      "grad_norm": 1.8086892366409302,
      "learning_rate": 9.871453138435082e-05,
      "loss": 4.5223,
      "step": 300
    },
    {
      "epoch": 0.08599607642901293,
      "grad_norm": 1.6665161848068237,
      "learning_rate": 9.828460877042133e-05,
      "loss": 4.4361,
      "step": 400
    },
    {
      "epoch": 0.10749509553626616,
      "grad_norm": 1.520196557044983,
      "learning_rate": 9.785468615649184e-05,
      "loss": 4.377,
      "step": 500
    },
    {
      "epoch": 0.1289941146435194,
      "grad_norm": 2.1534342765808105,
      "learning_rate": 9.742476354256234e-05,
      "loss": 4.329,
      "step": 600
    },
    {
      "epoch": 0.1504931337507726,
      "grad_norm": 1.5522505044937134,
      "learning_rate": 9.699484092863284e-05,
      "loss": 4.3053,
      "step": 700
    },
    {
      "epoch": 0.17199215285802585,
      "grad_norm": 1.535072684288025,
      "learning_rate": 9.656491831470335e-05,
      "loss": 4.2643,
      "step": 800
    },
    {
      "epoch": 0.1934911719652791,
      "grad_norm": 1.6178390979766846,
      "learning_rate": 9.613499570077386e-05,
      "loss": 4.2541,
      "step": 900
    },
    {
      "epoch": 0.2149901910725323,
      "grad_norm": 1.469452977180481,
      "learning_rate": 9.570507308684438e-05,
      "loss": 4.2266,
      "step": 1000
    },
    {
      "epoch": 0.23648921017978555,
      "grad_norm": 1.5605744123458862,
      "learning_rate": 9.527515047291487e-05,
      "loss": 4.2137,
      "step": 1100
    },
    {
      "epoch": 0.2579882292870388,
      "grad_norm": 1.5390288829803467,
      "learning_rate": 9.484522785898539e-05,
      "loss": 4.1828,
      "step": 1200
    },
    {
      "epoch": 0.279487248394292,
      "grad_norm": 2.1409239768981934,
      "learning_rate": 9.44153052450559e-05,
      "loss": 4.1788,
      "step": 1300
    },
    {
      "epoch": 0.3009862675015452,
      "grad_norm": 1.4005730152130127,
      "learning_rate": 9.398538263112641e-05,
      "loss": 4.1419,
      "step": 1400
    },
    {
      "epoch": 0.3224852866087985,
      "grad_norm": 1.465074062347412,
      "learning_rate": 9.355546001719691e-05,
      "loss": 4.1504,
      "step": 1500
    },
    {
      "epoch": 0.3439843057160517,
      "grad_norm": 1.522947072982788,
      "learning_rate": 9.312553740326742e-05,
      "loss": 4.1446,
      "step": 1600
    },
    {
      "epoch": 0.3654833248233049,
      "grad_norm": 1.4808021783828735,
      "learning_rate": 9.269561478933792e-05,
      "loss": 4.1457,
      "step": 1700
    },
    {
      "epoch": 0.3869823439305582,
      "grad_norm": 1.4065563678741455,
      "learning_rate": 9.226569217540843e-05,
      "loss": 4.0971,
      "step": 1800
    },
    {
      "epoch": 0.4084813630378114,
      "grad_norm": 1.46848726272583,
      "learning_rate": 9.183576956147893e-05,
      "loss": 4.1116,
      "step": 1900
    },
    {
      "epoch": 0.4299803821450646,
      "grad_norm": 1.4510653018951416,
      "learning_rate": 9.140584694754944e-05,
      "loss": 4.1114,
      "step": 2000
    },
    {
      "epoch": 0.45147940125231784,
      "grad_norm": 1.5660622119903564,
      "learning_rate": 9.097592433361996e-05,
      "loss": 4.0901,
      "step": 2100
    },
    {
      "epoch": 0.4729784203595711,
      "grad_norm": 1.3251572847366333,
      "learning_rate": 9.054600171969047e-05,
      "loss": 4.0833,
      "step": 2200
    },
    {
      "epoch": 0.4944774394668243,
      "grad_norm": 1.430375099182129,
      "learning_rate": 9.011607910576097e-05,
      "loss": 4.0554,
      "step": 2300
    },
    {
      "epoch": 0.5159764585740776,
      "grad_norm": 1.6432809829711914,
      "learning_rate": 8.968615649183148e-05,
      "loss": 4.0751,
      "step": 2400
    },
    {
      "epoch": 0.5374754776813307,
      "grad_norm": 1.4608793258666992,
      "learning_rate": 8.925623387790199e-05,
      "loss": 4.0497,
      "step": 2500
    },
    {
      "epoch": 0.558974496788584,
      "grad_norm": 1.5204122066497803,
      "learning_rate": 8.882631126397249e-05,
      "loss": 4.0563,
      "step": 2600
    },
    {
      "epoch": 0.5804735158958373,
      "grad_norm": 1.305800437927246,
      "learning_rate": 8.839638865004299e-05,
      "loss": 4.0692,
      "step": 2700
    },
    {
      "epoch": 0.6019725350030904,
      "grad_norm": 1.5141973495483398,
      "learning_rate": 8.79664660361135e-05,
      "loss": 4.0554,
      "step": 2800
    },
    {
      "epoch": 0.6234715541103437,
      "grad_norm": 1.5710450410842896,
      "learning_rate": 8.753654342218401e-05,
      "loss": 4.0494,
      "step": 2900
    },
    {
      "epoch": 0.644970573217597,
      "grad_norm": 1.3807547092437744,
      "learning_rate": 8.710662080825451e-05,
      "loss": 4.0141,
      "step": 3000
    },
    {
      "epoch": 0.6664695923248501,
      "grad_norm": 1.4114131927490234,
      "learning_rate": 8.667669819432502e-05,
      "loss": 4.0269,
      "step": 3100
    },
    {
      "epoch": 0.6879686114321034,
      "grad_norm": 1.4443771839141846,
      "learning_rate": 8.624677558039553e-05,
      "loss": 4.0064,
      "step": 3200
    },
    {
      "epoch": 0.7094676305393567,
      "grad_norm": 1.3984637260437012,
      "learning_rate": 8.581685296646605e-05,
      "loss": 4.0338,
      "step": 3300
    },
    {
      "epoch": 0.7309666496466098,
      "grad_norm": 1.4929181337356567,
      "learning_rate": 8.538693035253655e-05,
      "loss": 4.0076,
      "step": 3400
    },
    {
      "epoch": 0.7524656687538631,
      "grad_norm": 1.5041851997375488,
      "learning_rate": 8.495700773860706e-05,
      "loss": 4.014,
      "step": 3500
    },
    {
      "epoch": 0.7739646878611164,
      "grad_norm": 1.4175580739974976,
      "learning_rate": 8.452708512467757e-05,
      "loss": 3.998,
      "step": 3600
    },
    {
      "epoch": 0.7954637069683695,
      "grad_norm": 1.7966169118881226,
      "learning_rate": 8.409716251074807e-05,
      "loss": 3.9781,
      "step": 3700
    },
    {
      "epoch": 0.8169627260756228,
      "grad_norm": 1.377337098121643,
      "learning_rate": 8.366723989681857e-05,
      "loss": 3.987,
      "step": 3800
    },
    {
      "epoch": 0.8384617451828761,
      "grad_norm": 1.4473848342895508,
      "learning_rate": 8.323731728288908e-05,
      "loss": 3.9726,
      "step": 3900
    },
    {
      "epoch": 0.8599607642901292,
      "grad_norm": 1.5951286554336548,
      "learning_rate": 8.280739466895959e-05,
      "loss": 3.9877,
      "step": 4000
    },
    {
      "epoch": 0.8814597833973825,
      "grad_norm": 1.3582282066345215,
      "learning_rate": 8.23774720550301e-05,
      "loss": 3.9751,
      "step": 4100
    },
    {
      "epoch": 0.9029588025046357,
      "grad_norm": 1.3295422792434692,
      "learning_rate": 8.19475494411006e-05,
      "loss": 3.9912,
      "step": 4200
    },
    {
      "epoch": 0.9244578216118889,
      "grad_norm": 1.3881337642669678,
      "learning_rate": 8.151762682717111e-05,
      "loss": 3.9727,
      "step": 4300
    },
    {
      "epoch": 0.9459568407191422,
      "grad_norm": 1.3810762166976929,
      "learning_rate": 8.108770421324163e-05,
      "loss": 3.9841,
      "step": 4400
    },
    {
      "epoch": 0.9674558598263954,
      "grad_norm": 1.3465793132781982,
      "learning_rate": 8.065778159931214e-05,
      "loss": 3.9735,
      "step": 4500
    },
    {
      "epoch": 0.9889548789336486,
      "grad_norm": 1.4290145635604858,
      "learning_rate": 8.022785898538264e-05,
      "loss": 3.9604,
      "step": 4600
    },
    {
      "epoch": 1.0103195291714815,
      "grad_norm": 1.621107578277588,
      "learning_rate": 7.979793637145314e-05,
      "loss": 3.9315,
      "step": 4700
    },
    {
      "epoch": 1.0318185482787348,
      "grad_norm": 1.5303758382797241,
      "learning_rate": 7.936801375752365e-05,
      "loss": 3.9049,
      "step": 4800
    },
    {
      "epoch": 1.053317567385988,
      "grad_norm": 1.5282436609268188,
      "learning_rate": 7.893809114359416e-05,
      "loss": 3.915,
      "step": 4900
    },
    {
      "epoch": 1.0748165864932413,
      "grad_norm": 1.5366694927215576,
      "learning_rate": 7.850816852966466e-05,
      "loss": 3.8942,
      "step": 5000
    },
    {
      "epoch": 1.0963156056004946,
      "grad_norm": 1.7641927003860474,
      "learning_rate": 7.807824591573517e-05,
      "loss": 3.8868,
      "step": 5100
    },
    {
      "epoch": 1.1178146247077476,
      "grad_norm": 1.7233192920684814,
      "learning_rate": 7.764832330180568e-05,
      "loss": 3.8941,
      "step": 5200
    },
    {
      "epoch": 1.1393136438150009,
      "grad_norm": 1.58346688747406,
      "learning_rate": 7.72184006878762e-05,
      "loss": 3.903,
      "step": 5300
    },
    {
      "epoch": 1.1608126629222542,
      "grad_norm": 1.5959392786026,
      "learning_rate": 7.678847807394669e-05,
      "loss": 3.8899,
      "step": 5400
    },
    {
      "epoch": 1.1823116820295074,
      "grad_norm": 1.6566493511199951,
      "learning_rate": 7.63585554600172e-05,
      "loss": 3.901,
      "step": 5500
    },
    {
      "epoch": 1.2038107011367607,
      "grad_norm": 1.6001025438308716,
      "learning_rate": 7.592863284608772e-05,
      "loss": 3.9246,
      "step": 5600
    },
    {
      "epoch": 1.225309720244014,
      "grad_norm": 1.4707309007644653,
      "learning_rate": 7.549871023215822e-05,
      "loss": 3.8955,
      "step": 5700
    },
    {
      "epoch": 1.246808739351267,
      "grad_norm": 1.517409086227417,
      "learning_rate": 7.506878761822871e-05,
      "loss": 3.9015,
      "step": 5800
    },
    {
      "epoch": 1.2683077584585203,
      "grad_norm": 1.5890142917633057,
      "learning_rate": 7.463886500429923e-05,
      "loss": 3.9093,
      "step": 5900
    },
    {
      "epoch": 1.2898067775657736,
      "grad_norm": 1.5814589262008667,
      "learning_rate": 7.420894239036974e-05,
      "loss": 3.8862,
      "step": 6000
    },
    {
      "epoch": 1.3113057966730268,
      "grad_norm": 1.6468963623046875,
      "learning_rate": 7.377901977644024e-05,
      "loss": 3.9072,
      "step": 6100
    },
    {
      "epoch": 1.33280481578028,
      "grad_norm": 1.513866662979126,
      "learning_rate": 7.334909716251075e-05,
      "loss": 3.9076,
      "step": 6200
    },
    {
      "epoch": 1.3543038348875331,
      "grad_norm": 1.685776948928833,
      "learning_rate": 7.291917454858126e-05,
      "loss": 3.8845,
      "step": 6300
    },
    {
      "epoch": 1.3758028539947864,
      "grad_norm": 1.6625845432281494,
      "learning_rate": 7.248925193465177e-05,
      "loss": 3.8795,
      "step": 6400
    },
    {
      "epoch": 1.3973018731020397,
      "grad_norm": 1.5812101364135742,
      "learning_rate": 7.205932932072227e-05,
      "loss": 3.8935,
      "step": 6500
    },
    {
      "epoch": 1.418800892209293,
      "grad_norm": 1.552707552909851,
      "learning_rate": 7.162940670679278e-05,
      "loss": 3.8902,
      "step": 6600
    },
    {
      "epoch": 1.4402999113165462,
      "grad_norm": 1.5515620708465576,
      "learning_rate": 7.119948409286328e-05,
      "loss": 3.8926,
      "step": 6700
    },
    {
      "epoch": 1.4617989304237995,
      "grad_norm": 1.5010194778442383,
      "learning_rate": 7.07695614789338e-05,
      "loss": 3.8749,
      "step": 6800
    },
    {
      "epoch": 1.4832979495310528,
      "grad_norm": 1.5590276718139648,
      "learning_rate": 7.03396388650043e-05,
      "loss": 3.884,
      "step": 6900
    },
    {
      "epoch": 1.5047969686383058,
      "grad_norm": 1.4863429069519043,
      "learning_rate": 6.99097162510748e-05,
      "loss": 3.8592,
      "step": 7000
    },
    {
      "epoch": 1.526295987745559,
      "grad_norm": 1.5522340536117554,
      "learning_rate": 6.947979363714532e-05,
      "loss": 3.8686,
      "step": 7100
    },
    {
      "epoch": 1.5477950068528123,
      "grad_norm": 1.731824517250061,
      "learning_rate": 6.904987102321583e-05,
      "loss": 3.8588,
      "step": 7200
    },
    {
      "epoch": 1.5692940259600656,
      "grad_norm": 1.5931373834609985,
      "learning_rate": 6.861994840928633e-05,
      "loss": 3.8942,
      "step": 7300
    },
    {
      "epoch": 1.590793045067319,
      "grad_norm": 1.5633869171142578,
      "learning_rate": 6.819002579535684e-05,
      "loss": 3.864,
      "step": 7400
    },
    {
      "epoch": 1.612292064174572,
      "grad_norm": 1.52094304561615,
      "learning_rate": 6.776010318142735e-05,
      "loss": 3.8838,
      "step": 7500
    },
    {
      "epoch": 1.6337910832818254,
      "grad_norm": 1.5339574813842773,
      "learning_rate": 6.733018056749786e-05,
      "loss": 3.8738,
      "step": 7600
    },
    {
      "epoch": 1.6552901023890785,
      "grad_norm": 1.6285470724105835,
      "learning_rate": 6.690025795356836e-05,
      "loss": 3.8665,
      "step": 7700
    },
    {
      "epoch": 1.6767891214963317,
      "grad_norm": 1.7056270837783813,
      "learning_rate": 6.647033533963886e-05,
      "loss": 3.8649,
      "step": 7800
    },
    {
      "epoch": 1.698288140603585,
      "grad_norm": 1.6537096500396729,
      "learning_rate": 6.604041272570937e-05,
      "loss": 3.8531,
      "step": 7900
    },
    {
      "epoch": 1.719787159710838,
      "grad_norm": 1.5228052139282227,
      "learning_rate": 6.561049011177989e-05,
      "loss": 3.8732,
      "step": 8000
    },
    {
      "epoch": 1.7412861788180916,
      "grad_norm": 1.6491130590438843,
      "learning_rate": 6.518056749785038e-05,
      "loss": 3.8607,
      "step": 8100
    },
    {
      "epoch": 1.7627851979253446,
      "grad_norm": 1.5391695499420166,
      "learning_rate": 6.47506448839209e-05,
      "loss": 3.8635,
      "step": 8200
    },
    {
      "epoch": 1.7842842170325979,
      "grad_norm": 1.4175933599472046,
      "learning_rate": 6.432072226999141e-05,
      "loss": 3.8644,
      "step": 8300
    },
    {
      "epoch": 1.8057832361398511,
      "grad_norm": 1.4298069477081299,
      "learning_rate": 6.389079965606192e-05,
      "loss": 3.8547,
      "step": 8400
    },
    {
      "epoch": 1.8272822552471042,
      "grad_norm": 1.567419171333313,
      "learning_rate": 6.346087704213242e-05,
      "loss": 3.8788,
      "step": 8500
    },
    {
      "epoch": 1.8487812743543577,
      "grad_norm": 1.4738863706588745,
      "learning_rate": 6.303095442820293e-05,
      "loss": 3.8713,
      "step": 8600
    },
    {
      "epoch": 1.8702802934616107,
      "grad_norm": 1.6707499027252197,
      "learning_rate": 6.260103181427343e-05,
      "loss": 3.8705,
      "step": 8700
    },
    {
      "epoch": 1.891779312568864,
      "grad_norm": 1.515642762184143,
      "learning_rate": 6.217110920034394e-05,
      "loss": 3.8674,
      "step": 8800
    },
    {
      "epoch": 1.9132783316761173,
      "grad_norm": 1.596803069114685,
      "learning_rate": 6.174118658641444e-05,
      "loss": 3.8589,
      "step": 8900
    },
    {
      "epoch": 1.9347773507833705,
      "grad_norm": 1.7226476669311523,
      "learning_rate": 6.131126397248495e-05,
      "loss": 3.8527,
      "step": 9000
    },
    {
      "epoch": 1.9562763698906238,
      "grad_norm": 1.5970051288604736,
      "learning_rate": 6.0881341358555465e-05,
      "loss": 3.8586,
      "step": 9100
    },
    {
      "epoch": 1.9777753889978769,
      "grad_norm": 1.4625670909881592,
      "learning_rate": 6.0451418744625964e-05,
      "loss": 3.8601,
      "step": 9200
    },
    {
      "epoch": 1.9992744081051304,
      "grad_norm": 1.6547293663024902,
      "learning_rate": 6.0021496130696476e-05,
      "loss": 3.8757,
      "step": 9300
    },
    {
      "epoch": 2.020639058342963,
      "grad_norm": 1.6125072240829468,
      "learning_rate": 5.959157351676699e-05,
      "loss": 3.7758,
      "step": 9400
    },
    {
      "epoch": 2.0421380774502165,
      "grad_norm": 1.6786056756973267,
      "learning_rate": 5.916165090283749e-05,
      "loss": 3.7703,
      "step": 9500
    },
    {
      "epoch": 2.0636370965574695,
      "grad_norm": 1.9327545166015625,
      "learning_rate": 5.873172828890799e-05,
      "loss": 3.7591,
      "step": 9600
    },
    {
      "epoch": 2.085136115664723,
      "grad_norm": 1.6194781064987183,
      "learning_rate": 5.8301805674978504e-05,
      "loss": 3.7741,
      "step": 9700
    },
    {
      "epoch": 2.106635134771976,
      "grad_norm": 1.803894281387329,
      "learning_rate": 5.7871883061049016e-05,
      "loss": 3.7797,
      "step": 9800
    },
    {
      "epoch": 2.128134153879229,
      "grad_norm": 1.8250234127044678,
      "learning_rate": 5.744196044711953e-05,
      "loss": 3.7845,
      "step": 9900
    },
    {
      "epoch": 2.1496331729864826,
      "grad_norm": 1.7110884189605713,
      "learning_rate": 5.701203783319003e-05,
      "loss": 3.8031,
      "step": 10000
    },
    {
      "epoch": 2.1711321920937356,
      "grad_norm": 1.8307082653045654,
      "learning_rate": 5.658211521926053e-05,
      "loss": 3.7784,
      "step": 10100
    },
    {
      "epoch": 2.192631211200989,
      "grad_norm": 1.7953355312347412,
      "learning_rate": 5.6152192605331044e-05,
      "loss": 3.7833,
      "step": 10200
    },
    {
      "epoch": 2.214130230308242,
      "grad_norm": 1.6918038129806519,
      "learning_rate": 5.5722269991401556e-05,
      "loss": 3.7897,
      "step": 10300
    },
    {
      "epoch": 2.2356292494154952,
      "grad_norm": 1.749027132987976,
      "learning_rate": 5.5292347377472055e-05,
      "loss": 3.7882,
      "step": 10400
    },
    {
      "epoch": 2.2571282685227487,
      "grad_norm": 1.792297124862671,
      "learning_rate": 5.486242476354256e-05,
      "loss": 3.7759,
      "step": 10500
    },
    {
      "epoch": 2.2786272876300018,
      "grad_norm": 1.7139403820037842,
      "learning_rate": 5.443250214961307e-05,
      "loss": 3.7962,
      "step": 10600
    },
    {
      "epoch": 2.3001263067372553,
      "grad_norm": 1.8647069931030273,
      "learning_rate": 5.4002579535683585e-05,
      "loss": 3.7917,
      "step": 10700
    },
    {
      "epoch": 2.3216253258445083,
      "grad_norm": 1.8582773208618164,
      "learning_rate": 5.357265692175408e-05,
      "loss": 3.7699,
      "step": 10800
    },
    {
      "epoch": 2.343124344951762,
      "grad_norm": 1.9011216163635254,
      "learning_rate": 5.3142734307824595e-05,
      "loss": 3.7919,
      "step": 10900
    },
    {
      "epoch": 2.364623364059015,
      "grad_norm": 1.8385239839553833,
      "learning_rate": 5.27128116938951e-05,
      "loss": 3.8023,
      "step": 11000
    },
    {
      "epoch": 2.386122383166268,
      "grad_norm": 1.799201488494873,
      "learning_rate": 5.228288907996561e-05,
      "loss": 3.784,
      "step": 11100
    },
    {
      "epoch": 2.4076214022735214,
      "grad_norm": 1.8612000942230225,
      "learning_rate": 5.185296646603611e-05,
      "loss": 3.7754,
      "step": 11200
    },
    {
      "epoch": 2.4291204213807744,
      "grad_norm": 1.6845554113388062,
      "learning_rate": 5.1423043852106624e-05,
      "loss": 3.7678,
      "step": 11300
    },
    {
      "epoch": 2.450619440488028,
      "grad_norm": 1.9266600608825684,
      "learning_rate": 5.0993121238177136e-05,
      "loss": 3.7899,
      "step": 11400
    },
    {
      "epoch": 2.472118459595281,
      "grad_norm": 1.824047327041626,
      "learning_rate": 5.056319862424764e-05,
      "loss": 3.7878,
      "step": 11500
    },
    {
      "epoch": 2.493617478702534,
      "grad_norm": 1.824297308921814,
      "learning_rate": 5.013327601031814e-05,
      "loss": 3.7697,
      "step": 11600
    },
    {
      "epoch": 2.5151164978097875,
      "grad_norm": 1.6763708591461182,
      "learning_rate": 4.970335339638865e-05,
      "loss": 3.7756,
      "step": 11700
    },
    {
      "epoch": 2.5366155169170406,
      "grad_norm": 1.8980189561843872,
      "learning_rate": 4.9273430782459164e-05,
      "loss": 3.772,
      "step": 11800
    },
    {
      "epoch": 2.558114536024294,
      "grad_norm": 1.7847325801849365,
      "learning_rate": 4.884350816852967e-05,
      "loss": 3.7785,
      "step": 11900
    },
    {
      "epoch": 2.579613555131547,
      "grad_norm": 1.656722903251648,
      "learning_rate": 4.8413585554600175e-05,
      "loss": 3.8012,
      "step": 12000
    },
    {
      "epoch": 2.6011125742388,
      "grad_norm": 1.8493335247039795,
      "learning_rate": 4.798366294067068e-05,
      "loss": 3.771,
      "step": 12100
    },
    {
      "epoch": 2.6226115933460536,
      "grad_norm": 1.6755303144454956,
      "learning_rate": 4.7553740326741185e-05,
      "loss": 3.77,
      "step": 12200
    },
    {
      "epoch": 2.6441106124533067,
      "grad_norm": 1.9840036630630493,
      "learning_rate": 4.71238177128117e-05,
      "loss": 3.7806,
      "step": 12300
    },
    {
      "epoch": 2.66560963156056,
      "grad_norm": 1.705528974533081,
      "learning_rate": 4.66938950988822e-05,
      "loss": 3.7877,
      "step": 12400
    },
    {
      "epoch": 2.6871086506678132,
      "grad_norm": 1.7317014932632446,
      "learning_rate": 4.626397248495271e-05,
      "loss": 3.7718,
      "step": 12500
    },
    {
      "epoch": 2.7086076697750663,
      "grad_norm": 1.9225395917892456,
      "learning_rate": 4.583404987102321e-05,
      "loss": 3.7841,
      "step": 12600
    },
    {
      "epoch": 2.7301066888823198,
      "grad_norm": 2.0907866954803467,
      "learning_rate": 4.5404127257093726e-05,
      "loss": 3.7708,
      "step": 12700
    },
    {
      "epoch": 2.751605707989573,
      "grad_norm": 1.7101547718048096,
      "learning_rate": 4.497420464316423e-05,
      "loss": 3.7841,
      "step": 12800
    },
    {
      "epoch": 2.7731047270968263,
      "grad_norm": 1.8565393686294556,
      "learning_rate": 4.454428202923474e-05,
      "loss": 3.7754,
      "step": 12900
    },
    {
      "epoch": 2.7946037462040794,
      "grad_norm": 1.9922715425491333,
      "learning_rate": 4.411435941530525e-05,
      "loss": 3.7788,
      "step": 13000
    },
    {
      "epoch": 2.8161027653113324,
      "grad_norm": 1.6983873844146729,
      "learning_rate": 4.3684436801375754e-05,
      "loss": 3.7674,
      "step": 13100
    },
    {
      "epoch": 2.837601784418586,
      "grad_norm": 1.7077549695968628,
      "learning_rate": 4.325451418744626e-05,
      "loss": 3.7897,
      "step": 13200
    },
    {
      "epoch": 2.8591008035258394,
      "grad_norm": 1.814516544342041,
      "learning_rate": 4.282459157351677e-05,
      "loss": 3.7625,
      "step": 13300
    },
    {
      "epoch": 2.8805998226330924,
      "grad_norm": 1.7966886758804321,
      "learning_rate": 4.2394668959587276e-05,
      "loss": 3.7764,
      "step": 13400
    },
    {
      "epoch": 2.9020988417403455,
      "grad_norm": 1.8941456079483032,
      "learning_rate": 4.196474634565778e-05,
      "loss": 3.7784,
      "step": 13500
    },
    {
      "epoch": 2.923597860847599,
      "grad_norm": 1.7710657119750977,
      "learning_rate": 4.153482373172829e-05,
      "loss": 3.7765,
      "step": 13600
    },
    {
      "epoch": 2.945096879954852,
      "grad_norm": 1.817685842514038,
      "learning_rate": 4.11049011177988e-05,
      "loss": 3.7843,
      "step": 13700
    },
    {
      "epoch": 2.9665958990621055,
      "grad_norm": 1.9100922346115112,
      "learning_rate": 4.0674978503869305e-05,
      "loss": 3.7745,
      "step": 13800
    },
    {
      "epoch": 2.9880949181693586,
      "grad_norm": 1.7340385913848877,
      "learning_rate": 4.024505588993982e-05,
      "loss": 3.7856,
      "step": 13900
    },
    {
      "epoch": 3.0094595684071916,
      "grad_norm": 1.881994605064392,
      "learning_rate": 3.981513327601032e-05,
      "loss": 3.7412,
      "step": 14000
    },
    {
      "epoch": 3.0309585875144447,
      "grad_norm": 1.962680697441101,
      "learning_rate": 3.938521066208083e-05,
      "loss": 3.7179,
      "step": 14100
    },
    {
      "epoch": 3.0524576066216977,
      "grad_norm": 1.8826382160186768,
      "learning_rate": 3.895528804815133e-05,
      "loss": 3.7132,
      "step": 14200
    },
    {
      "epoch": 3.0739566257289512,
      "grad_norm": 2.0896081924438477,
      "learning_rate": 3.8525365434221845e-05,
      "loss": 3.7036,
      "step": 14300
    },
    {
      "epoch": 3.0954556448362043,
      "grad_norm": 1.9328532218933105,
      "learning_rate": 3.809544282029235e-05,
      "loss": 3.7046,
      "step": 14400
    },
    {
      "epoch": 3.1169546639434578,
      "grad_norm": 2.0869085788726807,
      "learning_rate": 3.7665520206362856e-05,
      "loss": 3.7092,
      "step": 14500
    },
    {
      "epoch": 3.138453683050711,
      "grad_norm": 2.1099977493286133,
      "learning_rate": 3.723559759243336e-05,
      "loss": 3.6839,
      "step": 14600
    },
    {
      "epoch": 3.159952702157964,
      "grad_norm": 1.8708323240280151,
      "learning_rate": 3.680567497850387e-05,
      "loss": 3.7075,
      "step": 14700
    },
    {
      "epoch": 3.1814517212652174,
      "grad_norm": 2.0617568492889404,
      "learning_rate": 3.637575236457438e-05,
      "loss": 3.6929,
      "step": 14800
    },
    {
      "epoch": 3.2029507403724704,
      "grad_norm": 2.203052282333374,
      "learning_rate": 3.594582975064489e-05,
      "loss": 3.7118,
      "step": 14900
    },
    {
      "epoch": 3.224449759479724,
      "grad_norm": 2.128002882003784,
      "learning_rate": 3.5515907136715396e-05,
      "loss": 3.7109,
      "step": 15000
    },
    {
      "epoch": 3.245948778586977,
      "grad_norm": 2.0591461658477783,
      "learning_rate": 3.50859845227859e-05,
      "loss": 3.704,
      "step": 15100
    },
    {
      "epoch": 3.2674477976942304,
      "grad_norm": 1.9877989292144775,
      "learning_rate": 3.465606190885641e-05,
      "loss": 3.697,
      "step": 15200
    },
    {
      "epoch": 3.2889468168014835,
      "grad_norm": 1.9502233266830444,
      "learning_rate": 3.422613929492691e-05,
      "loss": 3.7049,
      "step": 15300
    },
    {
      "epoch": 3.3104458359087365,
      "grad_norm": 2.0925827026367188,
      "learning_rate": 3.3796216680997424e-05,
      "loss": 3.7086,
      "step": 15400
    },
    {
      "epoch": 3.33194485501599,
      "grad_norm": 2.1450655460357666,
      "learning_rate": 3.336629406706793e-05,
      "loss": 3.7078,
      "step": 15500
    },
    {
      "epoch": 3.353443874123243,
      "grad_norm": 2.080415964126587,
      "learning_rate": 3.2936371453138435e-05,
      "loss": 3.7033,
      "step": 15600
    },
    {
      "epoch": 3.3749428932304966,
      "grad_norm": 2.0724844932556152,
      "learning_rate": 3.250644883920894e-05,
      "loss": 3.7277,
      "step": 15700
    },
    {
      "epoch": 3.3964419123377496,
      "grad_norm": 2.1108250617980957,
      "learning_rate": 3.207652622527945e-05,
      "loss": 3.7031,
      "step": 15800
    },
    {
      "epoch": 3.4179409314450027,
      "grad_norm": 2.0745623111724854,
      "learning_rate": 3.164660361134996e-05,
      "loss": 3.7202,
      "step": 15900
    },
    {
      "epoch": 3.439439950552256,
      "grad_norm": 2.038363456726074,
      "learning_rate": 3.121668099742047e-05,
      "loss": 3.7007,
      "step": 16000
    },
    {
      "epoch": 3.460938969659509,
      "grad_norm": 2.079153060913086,
      "learning_rate": 3.078675838349097e-05,
      "loss": 3.7198,
      "step": 16100
    },
    {
      "epoch": 3.4824379887667627,
      "grad_norm": 1.979784607887268,
      "learning_rate": 3.035683576956148e-05,
      "loss": 3.7241,
      "step": 16200
    },
    {
      "epoch": 3.5039370078740157,
      "grad_norm": 1.9714818000793457,
      "learning_rate": 2.9926913155631986e-05,
      "loss": 3.6868,
      "step": 16300
    },
    {
      "epoch": 3.525436026981269,
      "grad_norm": 1.9462846517562866,
      "learning_rate": 2.9496990541702495e-05,
      "loss": 3.7171,
      "step": 16400
    },
    {
      "epoch": 3.5469350460885223,
      "grad_norm": 1.9731409549713135,
      "learning_rate": 2.9067067927773e-05,
      "loss": 3.7195,
      "step": 16500
    },
    {
      "epoch": 3.5684340651957753,
      "grad_norm": 2.0485739707946777,
      "learning_rate": 2.8637145313843512e-05,
      "loss": 3.707,
      "step": 16600
    },
    {
      "epoch": 3.589933084303029,
      "grad_norm": 2.1238625049591064,
      "learning_rate": 2.8207222699914014e-05,
      "loss": 3.7041,
      "step": 16700
    },
    {
      "epoch": 3.611432103410282,
      "grad_norm": 2.0526959896087646,
      "learning_rate": 2.7777300085984526e-05,
      "loss": 3.7057,
      "step": 16800
    },
    {
      "epoch": 3.632931122517535,
      "grad_norm": 2.1571877002716064,
      "learning_rate": 2.734737747205503e-05,
      "loss": 3.7123,
      "step": 16900
    },
    {
      "epoch": 3.6544301416247884,
      "grad_norm": 1.8513559103012085,
      "learning_rate": 2.691745485812554e-05,
      "loss": 3.703,
      "step": 17000
    },
    {
      "epoch": 3.6759291607320415,
      "grad_norm": 2.127570867538452,
      "learning_rate": 2.6487532244196046e-05,
      "loss": 3.721,
      "step": 17100
    },
    {
      "epoch": 3.697428179839295,
      "grad_norm": 2.0457749366760254,
      "learning_rate": 2.6057609630266554e-05,
      "loss": 3.7174,
      "step": 17200
    },
    {
      "epoch": 3.718927198946548,
      "grad_norm": 1.949279546737671,
      "learning_rate": 2.562768701633706e-05,
      "loss": 3.7006,
      "step": 17300
    },
    {
      "epoch": 3.740426218053801,
      "grad_norm": 2.1383278369903564,
      "learning_rate": 2.519776440240757e-05,
      "loss": 3.7059,
      "step": 17400
    },
    {
      "epoch": 3.7619252371610545,
      "grad_norm": 2.2069692611694336,
      "learning_rate": 2.4767841788478074e-05,
      "loss": 3.7089,
      "step": 17500
    },
    {
      "epoch": 3.783424256268308,
      "grad_norm": 2.362515926361084,
      "learning_rate": 2.4337919174548582e-05,
      "loss": 3.6998,
      "step": 17600
    },
    {
      "epoch": 3.804923275375561,
      "grad_norm": 1.9646610021591187,
      "learning_rate": 2.3907996560619088e-05,
      "loss": 3.7125,
      "step": 17700
    },
    {
      "epoch": 3.826422294482814,
      "grad_norm": 2.0917913913726807,
      "learning_rate": 2.3478073946689596e-05,
      "loss": 3.7075,
      "step": 17800
    },
    {
      "epoch": 3.8479213135900676,
      "grad_norm": 2.5811872482299805,
      "learning_rate": 2.3048151332760105e-05,
      "loss": 3.7247,
      "step": 17900
    },
    {
      "epoch": 3.8694203326973207,
      "grad_norm": 2.161907196044922,
      "learning_rate": 2.261822871883061e-05,
      "loss": 3.7045,
      "step": 18000
    },
    {
      "epoch": 3.890919351804574,
      "grad_norm": 1.9746029376983643,
      "learning_rate": 2.218830610490112e-05,
      "loss": 3.7168,
      "step": 18100
    },
    {
      "epoch": 3.912418370911827,
      "grad_norm": 1.9785106182098389,
      "learning_rate": 2.1758383490971625e-05,
      "loss": 3.6925,
      "step": 18200
    },
    {
      "epoch": 3.9339173900190803,
      "grad_norm": 1.930976152420044,
      "learning_rate": 2.1328460877042133e-05,
      "loss": 3.7085,
      "step": 18300
    },
    {
      "epoch": 3.9554164091263337,
      "grad_norm": 1.9601346254348755,
      "learning_rate": 2.0898538263112642e-05,
      "loss": 3.6963,
      "step": 18400
    },
    {
      "epoch": 3.976915428233587,
      "grad_norm": 2.0716562271118164,
      "learning_rate": 2.0468615649183147e-05,
      "loss": 3.7071,
      "step": 18500
    },
    {
      "epoch": 3.9984144473408403,
      "grad_norm": 1.8723371028900146,
      "learning_rate": 2.0038693035253656e-05,
      "loss": 3.7024,
      "step": 18600
    },
    {
      "epoch": 4.019779097578673,
      "grad_norm": 2.4441330432891846,
      "learning_rate": 1.960877042132416e-05,
      "loss": 3.6584,
      "step": 18700
    },
    {
      "epoch": 4.041278116685926,
      "grad_norm": 2.126108407974243,
      "learning_rate": 1.917884780739467e-05,
      "loss": 3.6533,
      "step": 18800
    },
    {
      "epoch": 4.062777135793179,
      "grad_norm": 2.135991096496582,
      "learning_rate": 1.874892519346518e-05,
      "loss": 3.6451,
      "step": 18900
    },
    {
      "epoch": 4.084276154900433,
      "grad_norm": 2.1273717880249023,
      "learning_rate": 1.8319002579535684e-05,
      "loss": 3.654,
      "step": 19000
    },
    {
      "epoch": 4.105775174007686,
      "grad_norm": 2.3267886638641357,
      "learning_rate": 1.7889079965606193e-05,
      "loss": 3.6522,
      "step": 19100
    },
    {
      "epoch": 4.127274193114939,
      "grad_norm": 2.257111072540283,
      "learning_rate": 1.74591573516767e-05,
      "loss": 3.6468,
      "step": 19200
    },
    {
      "epoch": 4.148773212222192,
      "grad_norm": 2.2444353103637695,
      "learning_rate": 1.7029234737747207e-05,
      "loss": 3.6622,
      "step": 19300
    },
    {
      "epoch": 4.170272231329446,
      "grad_norm": 2.227121591567993,
      "learning_rate": 1.6599312123817716e-05,
      "loss": 3.6664,
      "step": 19400
    },
    {
      "epoch": 4.191771250436699,
      "grad_norm": 2.170128583908081,
      "learning_rate": 1.616938950988822e-05,
      "loss": 3.6382,
      "step": 19500
    },
    {
      "epoch": 4.213270269543952,
      "grad_norm": 2.410754680633545,
      "learning_rate": 1.573946689595873e-05,
      "loss": 3.6268,
      "step": 19600
    },
    {
      "epoch": 4.234769288651205,
      "grad_norm": 2.394681930541992,
      "learning_rate": 1.5309544282029235e-05,
      "loss": 3.675,
      "step": 19700
    },
    {
      "epoch": 4.256268307758458,
      "grad_norm": 2.276794910430908,
      "learning_rate": 1.4879621668099744e-05,
      "loss": 3.6467,
      "step": 19800
    },
    {
      "epoch": 4.277767326865712,
      "grad_norm": 2.135878324508667,
      "learning_rate": 1.444969905417025e-05,
      "loss": 3.6391,
      "step": 19900
    },
    {
      "epoch": 4.299266345972965,
      "grad_norm": 2.2282609939575195,
      "learning_rate": 1.4019776440240757e-05,
      "loss": 3.6432,
      "step": 20000
    },
    {
      "epoch": 4.320765365080218,
      "grad_norm": 2.492227792739868,
      "learning_rate": 1.3589853826311264e-05,
      "loss": 3.6572,
      "step": 20100
    },
    {
      "epoch": 4.342264384187471,
      "grad_norm": 2.3041985034942627,
      "learning_rate": 1.315993121238177e-05,
      "loss": 3.6476,
      "step": 20200
    },
    {
      "epoch": 4.363763403294724,
      "grad_norm": 2.220221996307373,
      "learning_rate": 1.2730008598452278e-05,
      "loss": 3.6587,
      "step": 20300
    },
    {
      "epoch": 4.385262422401978,
      "grad_norm": 2.182555675506592,
      "learning_rate": 1.2300085984522786e-05,
      "loss": 3.6617,
      "step": 20400
    },
    {
      "epoch": 4.406761441509231,
      "grad_norm": 2.1165177822113037,
      "learning_rate": 1.1870163370593295e-05,
      "loss": 3.6622,
      "step": 20500
    },
    {
      "epoch": 4.428260460616484,
      "grad_norm": 2.271653413772583,
      "learning_rate": 1.1440240756663802e-05,
      "loss": 3.6805,
      "step": 20600
    },
    {
      "epoch": 4.449759479723737,
      "grad_norm": 2.2107903957366943,
      "learning_rate": 1.1010318142734307e-05,
      "loss": 3.6539,
      "step": 20700
    },
    {
      "epoch": 4.4712584988309905,
      "grad_norm": 2.0447840690612793,
      "learning_rate": 1.0580395528804815e-05,
      "loss": 3.6563,
      "step": 20800
    },
    {
      "epoch": 4.492757517938244,
      "grad_norm": 2.3206892013549805,
      "learning_rate": 1.0150472914875323e-05,
      "loss": 3.6529,
      "step": 20900
    },
    {
      "epoch": 4.5142565370454975,
      "grad_norm": 2.288280487060547,
      "learning_rate": 9.72055030094583e-06,
      "loss": 3.671,
      "step": 21000
    },
    {
      "epoch": 4.5357555561527505,
      "grad_norm": 2.245767831802368,
      "learning_rate": 9.290627687016337e-06,
      "loss": 3.6478,
      "step": 21100
    },
    {
      "epoch": 4.5572545752600035,
      "grad_norm": 2.276057243347168,
      "learning_rate": 8.860705073086844e-06,
      "loss": 3.6627,
      "step": 21200
    },
    {
      "epoch": 4.578753594367257,
      "grad_norm": 2.396787643432617,
      "learning_rate": 8.430782459157351e-06,
      "loss": 3.6389,
      "step": 21300
    },
    {
      "epoch": 4.6002526134745105,
      "grad_norm": 2.1953654289245605,
      "learning_rate": 8.00085984522786e-06,
      "loss": 3.6566,
      "step": 21400
    },
    {
      "epoch": 4.621751632581764,
      "grad_norm": 2.3111391067504883,
      "learning_rate": 7.570937231298366e-06,
      "loss": 3.6502,
      "step": 21500
    },
    {
      "epoch": 4.643250651689017,
      "grad_norm": 2.431403636932373,
      "learning_rate": 7.141014617368874e-06,
      "loss": 3.6518,
      "step": 21600
    },
    {
      "epoch": 4.66474967079627,
      "grad_norm": 2.2541751861572266,
      "learning_rate": 6.711092003439381e-06,
      "loss": 3.6506,
      "step": 21700
    },
    {
      "epoch": 4.686248689903524,
      "grad_norm": 2.183366537094116,
      "learning_rate": 6.281169389509889e-06,
      "loss": 3.6513,
      "step": 21800
    },
    {
      "epoch": 4.707747709010777,
      "grad_norm": 2.232624053955078,
      "learning_rate": 5.851246775580395e-06,
      "loss": 3.6532,
      "step": 21900
    },
    {
      "epoch": 4.72924672811803,
      "grad_norm": 2.1833722591400146,
      "learning_rate": 5.421324161650903e-06,
      "loss": 3.6542,
      "step": 22000
    },
    {
      "epoch": 4.750745747225283,
      "grad_norm": 2.2405760288238525,
      "learning_rate": 4.99140154772141e-06,
      "loss": 3.6383,
      "step": 22100
    },
    {
      "epoch": 4.772244766332536,
      "grad_norm": 2.1741795539855957,
      "learning_rate": 4.561478933791917e-06,
      "loss": 3.645,
      "step": 22200
    },
    {
      "epoch": 4.793743785439789,
      "grad_norm": 2.3545238971710205,
      "learning_rate": 4.131556319862425e-06,
      "loss": 3.6431,
      "step": 22300
    },
    {
      "epoch": 4.815242804547043,
      "grad_norm": 2.2690651416778564,
      "learning_rate": 3.7016337059329323e-06,
      "loss": 3.6566,
      "step": 22400
    },
    {
      "epoch": 4.836741823654296,
      "grad_norm": 2.286040782928467,
      "learning_rate": 3.2717110920034398e-06,
      "loss": 3.6607,
      "step": 22500
    },
    {
      "epoch": 4.858240842761549,
      "grad_norm": 2.332930088043213,
      "learning_rate": 2.841788478073947e-06,
      "loss": 3.6559,
      "step": 22600
    },
    {
      "epoch": 4.879739861868802,
      "grad_norm": 2.436011552810669,
      "learning_rate": 2.4118658641444543e-06,
      "loss": 3.6618,
      "step": 22700
    },
    {
      "epoch": 4.901238880976056,
      "grad_norm": 2.310608386993408,
      "learning_rate": 1.9819432502149613e-06,
      "loss": 3.6495,
      "step": 22800
    },
    {
      "epoch": 4.922737900083309,
      "grad_norm": 2.273087501525879,
      "learning_rate": 1.5520206362854686e-06,
      "loss": 3.6555,
      "step": 22900
    },
    {
      "epoch": 4.944236919190562,
      "grad_norm": 2.1646556854248047,
      "learning_rate": 1.122098022355976e-06,
      "loss": 3.6716,
      "step": 23000
    }
  ],
  "logging_steps": 100,
  "max_steps": 23260,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0999575618823127e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
